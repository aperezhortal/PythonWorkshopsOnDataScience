{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Credits\n",
    "\n",
    "https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2018GL078202\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5798f0f0-ef68-4630-866c-c6c6cd459fa7",
    "_uuid": "20b6dcf907bd223863cbbe5f4995c284282b05d9"
   },
   "source": [
    "\n",
    "## Contents  \n",
    "<br>\n",
    "\n",
    "1. [Introduction to Neural Networks](#IntroductionToNN)\n",
    "\n",
    "    1. [What are Neural Networks](#WhatAreNN)\n",
    "    1. [Neurons](#Neurons)\n",
    "    1. [Activation Functions](#ActivationFunctions)\n",
    "    1. [ForwardPropagation](#ForwardPropagation)\n",
    "1. [Training Neural Networks](#TrainingNN)\n",
    "    1.  [Backward Propagation](#BackwardPropagation)\n",
    "1. [Neural networks using Keras](#KerasNN)\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required packages\n",
    "\n",
    "numpy\n",
    "dask\n",
    "xarray\n",
    "\n",
    "pip install nbresuse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "<a id='IntroductionToNN'></a>\n",
    "\n",
    "# Introduction to Neural Networks\n",
    "\n",
    "<a id='WhatAreNN'></a>\n",
    "## What are Neural Networks\n",
    "\n",
    "Neural networks are a type of machine learning models which are *losely inspired* in biological neurons and human nervous system. These models are used to recognize complex patterns and relationships that exists within a dataset. \n",
    "\n",
    "They have following properties:\n",
    "\n",
    "1. The core architecture of a Neural Network model consists of a large number of simple processing nodes called **Neurons** which are interconnected and organized in different layers. \n",
    "\n",
    "2. An individual node in a layer is connected to several other nodes in the previous and the next layer. The inputs form one layer are received and processed to generate the output which is passed to the next layer.\n",
    "\n",
    "3. The first layer of this architecture is often named as input layer which accepts the inputs, the last layer is named as the output layer which produces the output and every other layer between input and output layer is named is hidden layers. \n",
    "\n",
    "\n",
    "<img src=\"./fig/Neural_network_bottleneck_achitecture.svg\" alt=\"https://upload.wikimedia.org/wikipedia/commons/8/8b/Neural_network_bottleneck_achitecture.svg\" width=\"50%\">\n",
    "\n",
    "<a id='Neurons'></a>\n",
    "## Neurons\n",
    "\n",
    "A Neuron is a single processing unit of a Neural Network which are connected to different other neurons in the network. These connections repersents inputs and ouputs from a neuron. To each of its connections, the neuron assigns a “weight” (W) which signifies the importance the input and adds a bias (b) term. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png\" alt=\"https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png\" width=\"85%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ActivationFunctions'></a>\n",
    "### Activation Functions \n",
    "\n",
    "The activation functions are used to apply non-linear transformation on input to map it to output. The aim of activation functions is to predict the right class of the target variable based on the input combination of variables. Some of the popular activation functions are Relu, Sigmoid, and TanH. \n",
    "\n",
    "<img src=\"https://cs-cheatsheet.readthedocs.io/en/latest/_images/activation_functions.png\" alt=\"https://cs-cheatsheet.readthedocs.io/en/latest/_images/activation_functions.png\" width=\"70%\">\n",
    "\n",
    "< Source: Stanford cs231n >\n",
    "\n",
    "<a id='ForwardPropagation'></a>\n",
    "### Forward Propagation \n",
    "\n",
    "Neural Network model compute the obtain the desired ouput through a process called forward propagation, in which it passes the computed activation outputs in the forward direction. \n",
    "\n",
    "$o_j= \\varphi \\left( \\sum\\limits_{i=1}^n x_i*w_{ij} + b_i) \\right)$\n",
    "\n",
    "Where: \n",
    "\n",
    "- $x_i$ : $i^{th}$ input to the neuron.\n",
    "- *j* : the neuron number.\n",
    "- $w_{ij}$ : weight associated with the $i^(th}$ input.\n",
    "- $\\varphi$ : the activation function.\n",
    "- $b_i$ : the bias associated with the node for the $i^{th}$ input (neuron).\n",
    "- $o_j$ : output value for the $j^{th}$ neuron.\n",
    "\n",
    "Let's express the ouput values of a layer **O** in a more compact form using matrix notation:\n",
    "\n",
    "$\\vec{o}=\\varphi \\left( W \\cdot \\vec{x} + \\vec{b} \\right)$\n",
    "\n",
    "Where: \n",
    "\n",
    "- $\\vec{x}$ : Input vector (1D)\n",
    "- $W$ : weights for each neuron and input (2D).\n",
    "- $\\varphi$ : the activation function.\n",
    "- $\\vec{b}$ : the bias associated with the node for each input (1D)\n",
    "- $\\vec{o}$ : output values (1D)\n",
    "\n",
    "The arquitecture of the neural network and the weigths **W** and **b** represents our \"model\".\n",
    "But a priori we don't know the exact value of the weigths and biases for our particular problem.\n",
    "\n",
    "<a id='TrainingNN'></a>\n",
    "## Training Neural Networks\n",
    "\n",
    "To create a model that represents our data we adjust the weights gradually using a feedback signal from a set of know inputs and outputs, namely our training data.\n",
    "\n",
    "To adjust our weights, we need a measure of how well our model is performing in the training data. For that we will can use a metric called \"loss function\" or \"cost function\" to measure the distance between our predicted and the expected output.\n",
    "\n",
    "E.g.:\n",
    "\n",
    "$Loss = Cost function = RMSE(Actual\\_Values - Predicted\\_Values)$\n",
    "<a id='BackwardPropagation'></a>\n",
    "### Backward Propagation\n",
    "\n",
    "Neural Network model undergoes the process called backpropagation in which the error is passed to backward layers so that those layers can also improve the associated values of weights and bias. \n",
    "\n",
    "The backpropagation uses an algorithm called Gradient Descent in which the error is minimized and optimal values of weights and bias are obtained. This weights and bias adjustment is done by computing the derivative of error, derivative of weights, bias and subtracting them from the original values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='KerasNN'></a>\n",
    "\n",
    "# Neural networks using Keras\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "\n",
    "## Getting started: 30 seconds to Keras\n",
    "\n",
    "The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers.\n",
    "\n",
    "\n",
    "Let's use keras to create a neural network like the one shown in the figure:\n",
    "\n",
    "<img src=\"./fig/Neural_network_bottleneck_achitecture2.png?\" alt=\"https://upload.wikimedia.org/wikipedia/commons/8/8b/Neural_network_bottleneck_achitecture.svg\" width=\"50%\">\n",
    "\n",
    "\n",
    "Main characteristics:\n",
    "\n",
    "- Number of input variables: **7** \n",
    "- Number of input layer nodes (1st layer): **5** \n",
    "- Number of first hidden layer nodes (2nd layer): **4**\n",
    "- Number of second hidden layer nodes (3rd layer): **4**\n",
    "- Number of ouput layer nodes (3th layer): **3**\n",
    "- Activation functions\n",
    "\n",
    "    - input and output layer: **relu** \n",
    "    - hidden layers: **sigmoid**\n",
    "\n",
    "The Sequential model is a linear stack of layers.\n",
    "\n",
    "In keras, a regular feed-forwards densely-connected NN layer is represented by the \n",
    "[keras.layers.Dense](https://keras.io/layers/core/#dense) class.\n",
    "\n",
    "Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aperez/.conda/envs/python3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Stacking layers is as easy as .add():\n",
    "\n",
    "# Input layer. Here we specify the input shape.\n",
    "model.add(Dense(units=5, activation='relu', input_dim=7)) \n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(units=4, activation='sigmoid'))\n",
    "model.add(Dense(units=4, activation='sigmoid'))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(units=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the input shape\n",
    "\n",
    "The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape.\n",
    "\n",
    "\n",
    "### Compilation\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments:\n",
    "\n",
    "- An **optimizer**. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class. See: [optimizers](https://keras.io/optimizers/).\n",
    "- A **loss** function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. See: [losses](https://keras.io/losses/).\n",
    "- A list of **metrics**. For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function. [See available metrics here](https://keras.io/metrics/). The loss functions can also be used as metrics!\n",
    "\n",
    "Let's compile our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "abf6aa77-17cd-4b4a-bacf-903838fe813e",
    "_uuid": "735f9f4ab4893a28cc6a28f81a49a182fd02747d"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "# sgd = Stochastic gradient descent optimizer.             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function. Read its documentation [here](https://keras.io/models/sequential/).\n",
    "\n",
    "E.g.: Train the model, iterating on the data in batches of 32 samples\n",
    "\n",
    "`model.fit(x_data, y_data, epochs=10, batch_size=32)`\n",
    "\n",
    "\n",
    "Where: \n",
    "- **epochs**: Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch, epochs is to be understood as \"final epoch\". The training process may consist in more than one epoch.\n",
    "\n",
    "- **batch_size**: Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "\n",
    "### Predict\n",
    "\n",
    "The **predict** method generates output predictions for the input samples. \n",
    "\n",
    "`model.predict(x, batch_size=None, verbose=0, steps=None, callbacks=None)`\n",
    "\n",
    "The computation is done in batches.\n",
    "\n",
    "\n",
    "### evaluate\n",
    "\n",
    "The **evaluate**  method returns the loss value & metrics values for the model in test mode.\n",
    "\n",
    "`evaluate(x=None, y=None, batch_size=None, verbose=1)`\n",
    "\n",
    "Computation is done in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e249a1aa-a3df-4342-97ea-48a380d642a0",
    "_uuid": "3fa615d978d15f0430a3adf0bbe2b7ccc8205f5f"
   },
   "source": [
    "# Parametrize convection using Neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "34aa89c5-b5b7-4b34-9f16-0af4a4046cdf",
    "_uuid": "670c2f126fa04d98426502d720a6288e5b0cb5df",
    "collapsed": true
   },
   "source": [
    "Representing unresolved moist convection in coarse‐scale climate models remains one of the main bottlenecks of current climate simulations. The coarse resolution of the climate models is not sufficient to capture the convective processes that produce rain. In consequence, to represent the covection models rely on a parametrization that diagnose the precipitation at the gorund from the model prognostic variables (humidity, temperature, pressure, etc.).\n",
    "\n",
    "In this part of the tutorial we will try to use a Neural Network to learn the convective parametrization used in simplified climate model. \n",
    "\n",
    "\n",
    "## The model\n",
    "\n",
    "To generate the traning data (state variables and the precipitation) we will use the SPEEDY model.\n",
    "SPEEDY is a simplified GCM developed at ICTP by Franco Molteni and Fred Kucharski.\n",
    "\n",
    "The ICTP AGCM (nicknamed SPEEDY, for \"Simplified Parameterizations, privitivE-Equation DYnamics\") is based on a spectral dynamical core developed at the Geophysical Fluid Dynamics Laboratory. It is a hydrostatic, s-coordinate, spectral-transform model in the vorticity-divergence form, with semi-implicit treatment of gravity waves.\n",
    "\n",
    "- https://www.ictp.it/research/esp/models/speedy.aspx\n",
    "- Molteni F (2003) Atmospheric simulations using a GCM with simplified physical  parametrizations. I. Model climatology and variability in multi-decadal experiments. Clim Dyn 20: 175-191\n",
    "\n",
    "- Kucharski F, Molteni F, and Bracco A (2006) Decadal interactions between the western tropical Pacific and the North Atlantic Oscillation. Clim Dyn 26: 79-91\n",
    "\n",
    "## Training dataset\n",
    "\n",
    "The training dataset was generated by running the model for 10 years with a spatial resolution of approx. 2 degrees. The main characteristics of the dataset are:\n",
    "\n",
    "- T30 horizontal resoltution (approx. 2 degrees)\n",
    "- Outputs variables available every 6h for a 10 yr period\n",
    "- Only the latitudes between -30 and 30 degress are included\n",
    "\n",
    "Let's take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "dataset = xr.open_dataset('gcm_run_3.nc', chunks=dict(time = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'lon', 'lat', 'lev', 'gh', 'temp', 'q', 'sp', 'precnv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.variables.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- coordinates: 'time', 'lon', 'lat', 'lev'\n",
    "- prognostic variables: 'gh', 'temp', 'q', 'sp'\n",
    "- diagnostic variable: 'precnv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the times in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date:  2003-01-01T00:00:00.000000000\n",
      "Last date:  2003-12-31T18:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "print(\"First date: \" , dataset['time'].values.min())\n",
    "print(\"Last date: \" , dataset['time'].values.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see in more detail the contents of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (lat: 16, lev: 8, lon: 96, time: 1460)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2003-01-01 ... 2003-12-31T18:00:00\n",
       "  * lon      (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2\n",
       "  * lat      (lat) float64 -27.83 -24.12 -20.41 -16.7 ... 16.7 20.41 24.12 27.83\n",
       "  * lev      (lev) float64 925.0 850.0 700.0 500.0 300.0 200.0 100.0 30.0\n",
       "Data variables:\n",
       "    gh       (time, lev, lat, lon) float32 dask.array<shape=(1460, 8, 16, 96), chunksize=(100, 8, 16, 96)>\n",
       "    temp     (time, lev, lat, lon) float32 dask.array<shape=(1460, 8, 16, 96), chunksize=(100, 8, 16, 96)>\n",
       "    q        (time, lev, lat, lon) float32 dask.array<shape=(1460, 8, 16, 96), chunksize=(100, 8, 16, 96)>\n",
       "    sp       (time, lat, lon) float32 dask.array<shape=(1460, 16, 96), chunksize=(100, 16, 96)>\n",
       "    precnv   (time, lat, lon) float32 dask.array<shape=(1460, 16, 96), chunksize=(100, 16, 96)>\n",
       "Attributes:\n",
       "    CDI:          Climate Data Interface version 1.9.6 (http://mpimet.mpg.de/...\n",
       "    Conventions:  CF-1.6\n",
       "    history:      Wed May 08 08:57:29 2019: cdo -b F64 -f nc import_binary ex...\n",
       "    CDO:          Climate Data Operators version 1.9.6 (http://mpimet.mpg.de/..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In nutshell, the shape of the model prognostic variables (our predictors) :\n",
    "\n",
    "- __gh(time, lev, lat, lon)__ = **gh(1460, 8, 16, 96)** = geopotential height [m]\n",
    "- __temp(time, lev, lat, lon)__ = **temp(1460, 8, 16, 96)** = abs. temperature [degK]\n",
    "- __q(time, lev, lat, lon)__ = **q(1460, 8, 16, 96)** = specific humidity [g/Kg]\n",
    "- __sp(time, lat, lon)__ = **sp(1460, 16, 96)** = surface pressure [hPa]\n",
    "\n",
    "The shape of the diagnosed precipitation (the predicted variable) is:\n",
    "- __precnv(time, lat, lon)__ = **precnv(1460, 16, 96)** = convective precipitation [mm/day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove the unnecessary attributes to have cleaner outputs\n",
    "dataset.attrs=dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "We are going to predict the precipitation at each grid point using the information from the predictor variables in the corresponding column.\n",
    "\n",
    "Therefore, the input variables to the Neural network consist of\n",
    "\n",
    "- __Input__: $\\vec{x}= \\left\\{ gh(z), temp(z), q(z), sp \\right\\}$, with a length of $\\vec{x}$ is 8+8+8+1=25.\n",
    "- __Output__: R, the precipitation at the ground with length=1.\n",
    "\n",
    "The training samples in the dataset consist in many columns for every time step. Hence, based on the amount of data in dataset we have:\n",
    "\n",
    "$total\\_samples=number\\_of\\_times*number\\_of\\_lats*number\\_of\\_lons$\n",
    "\n",
    "$total\\_samples=1460*16*96=2242560$\n",
    "\n",
    "**That is more than 2 millon samples per year!**\n",
    "\n",
    "\n",
    "### Step 1: Collapse dimensions \n",
    "\n",
    "In the original dataset the dimensions of our variables are (time, lev, lat, lon) or (time, lat, lon).\n",
    "What we want to feed to the Neural network are arrays with shape x=(samples, variables) and y=(samples).\n",
    "\n",
    "Therefore, we have to collapse the (time, lat, lon) dimensions into a single dimension (sample).\n",
    "\n",
    "We will create a function to do that for a given dataset and save the results into a file for future use.\n",
    "Saving the preprocessed data to a file will also help to mainting the memory requirements to the minimum.\n",
    "\n",
    "But, before we create the function, let's see this how this preprocessing is done in a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = dataset.isel(time=slice(1,101))\n",
    "dict(small_dataset.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collapse the (time, lat, lon) dimensions into a single dimension (sample) we will use the [Dataset's stack](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.stack.html#xarray.Dataset.stack) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_dataset = small_dataset.stack(sample=('time', 'lat', 'lon'))\n",
    "stacked_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reorder the variables dimensions as (sample, lev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_dataset = stacked_dataset.transpose('sample', 'lev')\n",
    "stacked_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create the x,y training samples, where their dimensions are:\n",
    "- x(samples, variables)\n",
    "- y(samples)\n",
    "\n",
    "The aggregate all the column data from all the variables into a single dimension, we will use the [numpy's append](https://docs.scipy.org/doc/numpy/reference/generated/numpy.append.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of dimensions of x are x(samples, variables)\n",
    "\n",
    "# \"sp\" dimensions were (time, lat, lon). After we collapse these dimensions into \"sample\"\n",
    "# it became a 1D variable.\n",
    "\n",
    "# To start appending all the column data into a the dimension variable,\n",
    "# let's reshape sp to (samples, 1)\n",
    "x = stacked_dataset['sp'].values[:,None]\n",
    "# (None is the same as np.newaxis) \n",
    "\n",
    "# Now, let's continue adding the data into the second dimension\n",
    "for variable in ['gh', 'temp', 'q']:\n",
    "    x=np.append(x, stacked_dataset[variable].values, axis=1)\n",
    "\n",
    "y = stacked_dataset['precnv'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x.shape=', x.shape)\n",
    "print('y.shape=', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = xr.Dataset({'x': (['samples', 'variables'],  x),\n",
    "                          'y': (['samples'], y)} )\n",
    "\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function that implements the latter preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_dims(input_dataset_path, output_dataset_path):\n",
    "    \"\"\"\n",
    "    Prepare the training samples from the input dataset for NN feeding. \n",
    "    \n",
    "    x=(samples, variables) and y=(samples).\n",
    "    \"\"\"\n",
    "    \n",
    "    _input_dataset = xr.open_dataset(input_dataset_path)\n",
    "    \n",
    "    _stacked_dataset = _input_dataset.stack(sample=('time', 'lat', 'lon'))\n",
    "    \n",
    "    _stacked_dataset = _stacked_dataset.transpose('sample', 'lev')\n",
    "    \n",
    "    _x = _stacked_dataset['sp'].values[:,None]\n",
    "    for variable in ['gh', 'temp', 'q']:\n",
    "        _x=np.append(_x, _stacked_dataset[variable].values, axis=1)\n",
    "\n",
    "    _y = _stacked_dataset['precnv'].values\n",
    "    _new_dataset = xr.Dataset({'x': (['samples', 'variables'],  _x),\n",
    "                             'y': (['samples'], _y)} )\n",
    "    \n",
    "    \n",
    "    #Save dataset\n",
    "    _new_dataset.to_netcdf(output_dataset_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process all the input files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_dims('gcm_run_3.nc', 'train_3.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly, working with the entire dataset is not as easy, specially in computers with low ram memory.\n",
    "\n",
    "To keep the memory requirements to the miminum, let's read our dataset as a sequence of data, that is, several (but not all) samples at a time.\n",
    "\n",
    "For that we will make use of the Kera's [Sequence](https://keras.io/utils/#sequence) class.\n",
    "The **Sequence** are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch.\n",
    "\n",
    "When we define a custom Sequence for our problem, the Sequence must implement the \\_\\_getitem\\_\\_ and the \\_\\_len\\_\\_ methods.\n",
    "The method \\_\\_getitem\\_\\_ should return a complete batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #: Lazy loading of all the preprocessed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = xr.open_mfdataset(['train_3.nc'])\n",
    "\n",
    "\n",
    "# i_sam = input_dataset.samples[(1<input_dataset['y']) & (input_dataset['y']<100)]\n",
    "\n",
    "# input_dataset = input_dataset.isel(samples=i_sam)\n",
    "input_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(input_dataset['x'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a neural network\n",
    "\n",
    "\n",
    "### Separate the dataset in train and validation subsets\n",
    "\n",
    "Let's select the first 4/5 of the times for training and the last 1/5 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_data = input_scaler.transform(input_dataset['x'].values)\n",
    "y_data = input_dataset['y'].values\n",
    "\n",
    "(x_train, x_test, \n",
    " y_train, y_test) = train_test_split(x_data,\n",
    "                                     y_data,\n",
    "                                     test_size = 0.25,# Keep 25% of the samples for testing \n",
    "                                     shuffle=False) # Shuffle the samples\n",
    "\n",
    "# x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Neural Network\n",
    "\n",
    "Let's construct a Neural Network with the following layers:\n",
    "\n",
    "- Input: $\\vec{x}$ (length 25)\n",
    "- Output: R (length 1).\n",
    "- Input layer (1st layer): **10 nodes, relu activation function** \n",
    "- 1st hidden layer (2nd layer): **10 nodes, sigmoid activation function** \n",
    "- 2nd hidden layer (3rd layer): **5 nodes, sigmoid activation function** \n",
    "- Output layer (4th layer): **1 nodes, relu activation function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "my_model = Sequential()\n",
    "# Input layer. Here we specify the input shape.\n",
    "my_model.add(Dense(units=25, activation='relu', input_dim=25)) \n",
    "# Hidden layers\n",
    "my_model.add(Dense(units=25, activation='relu'))\n",
    "#my_model.add(Dense(units=25, activation='relu'))\n",
    "#Output layer\n",
    "my_model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "# Compile the model\n",
    "my_model.compile(optimizer='Adam', loss='mse', metrics=['rmse'])\n",
    "\n",
    "\n",
    "# Talk more about the options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Since we are using the keras Sequence to generate the data, instead of using the **fit** method, we will use \n",
    "the [fit_generator](https://keras.io/models/model/#fit_generator) method.\n",
    "\n",
    "**fit_generator** trains the model on data generated batch-by-batch by a Python generator (or an instance of Sequence). The generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU.\n",
    "\n",
    "The use of keras.utils.Sequence guarantees the ordering and guarantees the single use of every input per epoch when using use_multiprocessing=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model.fit_generator(train_seq, epochs=1, use_multiprocessing=True, workers=2,\n",
    "#                        validation_data=validation_seq, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit(x=x_train,y=y_train, validation_data=(x_test,y_test), epochs=20, verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e8d53f14-be0d-4b95-bc38-8714ec0239a1",
    "_uuid": "d5209b969be98b15aa0aa3e5923f88174fc8d7fb"
   },
   "source": [
    "# Credits\n",
    "\n",
    "The description of the Keras library was adapted from the Keras official documentation at https://keras.io/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "### Mit License\n",
    "\n",
    "Copyright 2019 Andres Perez Hortal\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
