{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cwpg0S7p4Qdq"
   },
   "source": [
    "# Credits\n",
    "\n",
    "- The description of the Keras library was adapted from the Keras official documentation at https://keras.io/.\n",
    "\n",
    "- Gentine, P., Pritchard, M., Rasp, S., Reinaudi, G., & Yacalis, G. ( 2018). Could machine learning break the convection parameterization deadlock? Geophysical Research Letters, 45, 5742– 5751. \n",
    "https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2018GL078202\n",
    "\n",
    "\n",
    "# 7 - Parameterize Convection using Neural Networks\n",
    "\n",
    "Representing unresolved moist convection in coarse‐scale climate models remains one of the main challenges of current climate simulations. The coarse resolution of these models is not sufficient to resolve the convective processes that produce rain.\n",
    "In consequence, to represent the convection models rely on a parametrization that diagnoses the precipitation at the ground from the model prognostic variables (humidity, temperature, pressure, etc.).\n",
    "\n",
    "In this tutorial we will show how the convective parametrizations in a simple numerical climate model can be modeled using Neural Networks. The training and testing data (model state variables and the precipitation) were created using the SPEEDY model.\n",
    "SPEEDY is a simplified GCM developed at ICTP by Franco Molteni and Fred Kucharski. The ICTP AGCM (nicknamed SPEEDY, for \"Simplified Parameterizations, privitivE-Equation DYnamics\") is based on a spectral dynamical core developed at the Geophysical Fluid Dynamics Laboratory. It is a hydrostatic, s-coordinate, spectral-transform model in the vorticity-divergence form, with semi-implicit treatment of gravity waves.\n",
    "\n",
    "In the following sections, we will cover the Neural Network basics and then create and train a Neural Network using the Keras python library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5798f0f0-ef68-4630-866c-c6c6cd459fa7",
    "_uuid": "20b6dcf907bd223863cbbe5f4995c284282b05d9",
    "colab_type": "text",
    "id": "tQFMYQtI4Qdx"
   },
   "source": [
    "\n",
    "## Contents  \n",
    "<br>\n",
    "\n",
    "1. [Introduction to Neural Networks](#IntroductionToNN)\n",
    "\n",
    "    1. [What are Neural Networks](#WhatAreNN)\n",
    "    1. [Neurons](#Neurons)\n",
    "    1. [Activation Functions](#ActivationFunctions)\n",
    "    1. [ForwardPropagation](#ForwardPropagation)\n",
    "    1. [Training Neural Networks](#TrainingNN)\n",
    "        1.[Getting started: 30 seconds to Keras](#GettingStartedToKeras)\n",
    "1. [Parameterize convection using Neural networks](#ParameterizeConvection)\n",
    "    1. [The climate model](#Speedy)\n",
    "    1. [Training dataset](#Dataset)\n",
    "    1. [Preparing the data](#Preprocessing)\n",
    "    1. [Train my first neural network](#MyFirstNN)\n",
    "    1. [Optimal neural network architecture and training parameters](#OptimalModel)\n",
    "    \n",
    "1. Credits\n",
    "1. License\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ee_2g10XZuL"
   },
   "source": [
    "# Requirements \n",
    "\n",
    "This notebook was created to be used in google Colab, reading the input data files from Google Drive. \n",
    "That means that you need a google account to follow this notebook as it is. \n",
    "\n",
    "## Google Colab users \n",
    "\n",
    "Assuming that you already have a google account, to set up the environment needed for this tutorial follow these steps:\n",
    "\n",
    "- In your Google Drive, create a new directory in the root drive's root called \"Datasets\" \n",
    "- Upload the following files into that directory: https://drive.google.com/open?id=1tDGUqrkMWCSTQALJ28BdGqh70aw7i8nS\n",
    "- Go to https://colab.research.google.com/ and upload this notebook (Python3)\n",
    "\n",
    "Almost all the dependencies are already installed in the google colab environment. You are ready to go now! \n",
    "\n",
    "## Running jupyter notebook locally\n",
    "\n",
    "If you want to run this notebook in your local computer, you need to download the files in https://drive.google.com/open?id=1tDGUqrkMWCSTQALJ28BdGqh70aw7i8nS and update the paths in this notebook to point to your local directories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab_type": "text",
    "id": "7PE6Y-GI4Qd6"
   },
   "source": [
    "<a id='IntroductionToNN'></a>\n",
    "\n",
    "# Introduction to Neural Networks\n",
    "\n",
    "<a id='WhatAreNN'></a>\n",
    "## What are Neural Networks?\n",
    "\n",
    "Neural networks are a type of machine learning models which are *loosely inspired* in biological neurons and human nervous system. These models are used to recognize complex patterns and relationships that exist within a dataset. \n",
    "\n",
    "They have the following properties:\n",
    "\n",
    "1. The core architecture of a Neural Network model consists of a large number of simple processing nodes called **Neurons** which are interconnected and organized in different layers. \n",
    "\n",
    "2. An individual node in a layer is connected to several other nodes in the previous and the next layer. The inputs form one layer are received and processed to generate the output which is passed to the next layer.\n",
    "\n",
    "3. The first layer of this architecture is often named as input layer which accepts the inputs, the last layer is named as the output layer which produces the output and every other layer between input and output layer is named is hidden layers. \n",
    "\n",
    "\n",
    "<img src=\"https://github.com/aperezhortal/PythonWorkshopsOnDataScience/raw/master/fig/Neural_network_bottleneck_achitecture.png\" alt=\"https://github.com/aperezhortal/PythonWorkshopsOnDataScience/raw/master/fig/Neural_network_bottleneck_achitecture2.png\" width=\"50%\">\n",
    "\n",
    "<a id='Neurons'></a>\n",
    "## Neurons\n",
    "\n",
    "A Neuron is a single processing unit of a Neural Network which is connected to different other neurons in the network. These connections repersents inputs and ouputs from a neuron. To each of its connections, the neuron assigns a “weight” (W) which signifies the importance the input and adds a bias (b) term. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png\" alt=\"https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png\" width=\"85%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wb9giRg74Qd-"
   },
   "source": [
    "<a id='ActivationFunctions'></a>\n",
    "### Activation Functions \n",
    "\n",
    "The activation functions are used to apply a non-linear transformation on the input to map it to output. The aim of activation functions is to predict the right class of the target variable based on the input combination of variables. Some of the popular activation functions are Relu, Sigmoid, and TanH. \n",
    "\n",
    "<img src=\"https://cs-cheatsheet.readthedocs.io/en/latest/_images/activation_functions.png\" alt=\"https://cs-cheatsheet.readthedocs.io/en/latest/_images/activation_functions.png\" width=\"70%\">\n",
    "\n",
    "< Source: Stanford cs231n >\n",
    "\n",
    "<a id='ForwardPropagation'></a>\n",
    "### Forward Propagation \n",
    "\n",
    "Neural Network model obtains the desired output through a process called forward propagation, in which it passes the computed activation outputs in the forward direction. \n",
    "\n",
    "$o_j= \\varphi \\left( \\sum\\limits_{i=1}^n x_i*w_{ij} + b_i) \\right)$\n",
    "\n",
    "Where: \n",
    "\n",
    "- $x_i$ : $i^{th}$ input to the neuron.\n",
    "- *j* : the neuron number.\n",
    "- $w_{ij}$ : weight associated with the $i^{th}$ input.\n",
    "- $\\varphi$ : the activation function.\n",
    "- $b_i$ : the bias associated with the node for the $i^{th}$ input (neuron).\n",
    "- $o_j$ : output value for the $j^{th}$ neuron.\n",
    "\n",
    "Let's express the ouput values of a layer **O** in a more compact form using matrix notation:\n",
    "\n",
    "$\\vec{o}=\\varphi \\left( W \\cdot \\vec{x} + \\vec{b} \\right)$\n",
    "\n",
    "Where: \n",
    "\n",
    "- $\\vec{x}$ : Input vector (1D)\n",
    "- $W$ : weights for each neuron and input (2D).\n",
    "- $\\varphi$ : the activation function.\n",
    "- $\\vec{b}$ : the bias associated with the node for each input (1D)\n",
    "- $\\vec{o}$ : output values (1D)\n",
    "\n",
    "The architecture of the neural network and the weights **W** and **b** represents our \"model\".\n",
    "But a priori we don't know the exact value of the weights and biases for our particular problem.\n",
    "\n",
    "<a id='TrainingNN'></a>\n",
    "## Training Neural Networks\n",
    "\n",
    "To create a model that represents our data we adjust the weights gradually using a feedback signal from a set of known inputs and outputs, namely our training data.\n",
    "\n",
    "To adjust our weights, we need a measure of how well our model is performing in the training data. For that, we will use a metric called \"loss function\" or \"cost function\" to measure the distance between our predicted and the expected output.\n",
    "\n",
    "E.g.:\n",
    "\n",
    "$Loss = Cost function = RMSE(Actual\\_Values - Predicted\\_Values)$\n",
    "<a id='BackwardPropagation'></a>\n",
    "### Backward Propagation\n",
    "\n",
    "Neural Network model undergoes the process called backpropagation in which the error is passed to backward layers so that those layers can also improve the associated values of weights and bias. \n",
    "\n",
    "The backpropagation uses an algorithm called Gradient Descent in which the error is minimized and optimal values of weights and bias are obtained. This weights and bias adjustment is done by computing the derivative of error, the derivative of weights, bias and subtracting them from the original values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJ97MPa74QeC"
   },
   "source": [
    "<a id='KerasNN'></a>\n",
    "\n",
    "## Neural networks using Keras\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "\n",
    "### Getting started: 30 seconds to Keras\n",
    "\n",
    "The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows building arbitrary graphs of layers.\n",
    "\n",
    "\n",
    "Let's use keras to create a neural network like the one shown in the figure:\n",
    "\n",
    "<img src=\"https://github.com/aperezhortal/PythonWorkshopsOnDataScience/raw/master/fig/Neural_network_bottleneck_achitecture2.png\" alt=\"https://github.com/aperezhortal/PythonWorkshopsOnDataScience/raw/master/fig/Neural_network_bottleneck_achitecture2.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "Main characteristics:\n",
    "\n",
    "- Number of input variables: **7** \n",
    "- Number of input layer nodes (1st layer): **5** \n",
    "- Number of first hidden layer nodes (2nd layer): **4**\n",
    "- Number of second hidden layer nodes (3rd layer): **4**\n",
    "- Number of ouput layer nodes (4th layer): **3**\n",
    "- Activation functions\n",
    "\n",
    "    - input and output layer: **relu** \n",
    "    - hidden layers: **sigmoid**\n",
    "\n",
    "The Sequential model is a linear stack of layers.\n",
    "\n",
    "In keras, a regular feed-forwards densely-connected NN layer is represented by the \n",
    "[keras.layers.Dense](https://keras.io/layers/core/#dense) class.\n",
    "\n",
    "Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, the kernel is a weights matrix created by the layer, and the bias is a bias vector created by the layer (only applicable if use_bias is True).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzE1goId4QeG",
    "outputId": "f41f148d-521c-4075-a091-30ff1d435ccb"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Stacking layers is as easy as .add():\n",
    "\n",
    "# Input layer. Here we specify the input shape.\n",
    "model.add(Dense(units=5, activation='relu', input_dim=7)) \n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(units=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q1DmDMKV4QeV"
   },
   "source": [
    "### Specifying the input shape\n",
    "\n",
    "The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because the following layers can do automatic shape inference) needs to receive information about its input shape.\n",
    "\n",
    "\n",
    "### Compilation\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments:\n",
    "\n",
    "- An **optimizer**. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class. See: [optimizers](https://keras.io/optimizers/).\n",
    "- A **loss** function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. See: [losses](https://keras.io/losses/).\n",
    "- A list of **metrics**. For any classification problem, you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function. [See available metrics here](https://keras.io/metrics/). The loss functions can also be used as metrics!\n",
    "\n",
    "Let's compile our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "abf6aa77-17cd-4b4a-bacf-903838fe813e",
    "_uuid": "735f9f4ab4893a28cc6a28f81a49a182fd02747d",
    "colab": {},
    "colab_type": "code",
    "id": "X1eaddCE4QeY"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "# sgd = Stochastic gradient descent optimizer.             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrVzoU4D4Qef"
   },
   "source": [
    "### Training\n",
    "\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function. Read its documentation [here](https://keras.io/models/sequential/).\n",
    "\n",
    "E.g.: Train the model, iterating on the data in batches of 32 samples\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "`history = model.fit(x_data, y_data, epochs=10, batch_size=32)`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Where: \n",
    "- **epochs**: Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch, epochs is to be understood as \"final epoch\". The training process may consist in more than one epoch.\n",
    "\n",
    "- **batch_size**: Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "\n",
    "- **history**:  A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "\n",
    "### Predict\n",
    "\n",
    "The **predict** method generates output predictions for the input samples. \n",
    "\n",
    "---\n",
    "\n",
    "`predicted_values = model.predict(x, batch_size=None, verbose=0, steps=None, callbacks=None)`\n",
    "\n",
    "---\n",
    "\n",
    "The computation is done in batches.\n",
    "\n",
    "\n",
    "### evaluate\n",
    "\n",
    "The **evaluate**  method returns the loss value & metrics values for the model in test mode.\n",
    "\n",
    "---\n",
    "\n",
    "`evaluate(x=None, y=None, batch_size=None, verbose=1)`\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Now, let's use Keras to parameterize convection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e249a1aa-a3df-4342-97ea-48a380d642a0",
    "_uuid": "3fa615d978d15f0430a3adf0bbe2b7ccc8205f5f",
    "colab_type": "text",
    "id": "_GgtueFQ4Qek"
   },
   "source": [
    "# Parameterize convection using Neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "34aa89c5-b5b7-4b34-9f16-0af4a4046cdf",
    "_uuid": "670c2f126fa04d98426502d720a6288e5b0cb5df",
    "colab_type": "text",
    "collapsed": true,
    "id": "NK0nsk5w4Qem"
   },
   "source": [
    "Representing unresolved moist convection in coarse‐scale climate models remains one of the main challenges of current climate simulations. The coarse resolution of these models is not sufficient to resolve the convective processes that produce rain.\n",
    "In consequence, to represent the convection models rely on a parametrization that diagnoses the precipitation at the ground from the model prognostic variables (humidity, temperature, pressure, etc.).\n",
    "\n",
    "In this part of the tutorial, we will try to use a Neural Network to learn the convective parametrization used a simplified climate model. \n",
    "\n",
    "## The model\n",
    "\n",
    "The training data (state variables and the precipitation) was created using the SPEEDY model.\n",
    "\n",
    "The ICTP AGCM (nicknamed SPEEDY, for \"Simplified Parameterizations, privitivE-Equation DYnamics\") is based on a spectral dynamical core developed at the Geophysical Fluid Dynamics Laboratory. It is a hydrostatic, s-coordinate, spectral-transform model in the vorticity-divergence form, with semi-implicit treatment of gravity waves.\n",
    "\n",
    "- https://www.ictp.it/research/esp/models/speedy.aspx\n",
    "- Molteni F (2003) Atmospheric simulations using a GCM with simplified physical  parametrizations. I. Model climatology and variability in multi-decadal experiments. Clim Dyn 20: 175-191\n",
    "\n",
    "- Kucharski F, Molteni F, and Bracco A (2006) Decadal interactions between the western tropical Pacific and the North Atlantic Oscillation. Clim Dyn 26: 79-91\n",
    "\n",
    "## Training dataset\n",
    "\n",
    "The training dataset was generated by running the model for 10 years with a spatial resolution of approx. 2 degrees. The main characteristics of the dataset are:\n",
    "\n",
    "- T30 horizontal resolution (approx. 2 degrees)\n",
    "- Outputs variables available every 6h for a 10 yr period\n",
    "- Only the latitudes between -30 and 30 degrees are included\n",
    "\n",
    "Let's first take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "AfTwoQVB4UgQ",
    "outputId": "1a56a0be-bfc1-4a8f-ae01-09ff531a863b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jCDvG5L04iPr",
    "outputId": "a66cd030-5f8f-48cf-8a84-7d7a075c6b9c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('/content/gdrive/My Drive/Datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "0DEXQNaL5K-Q",
    "outputId": "fc679a21-d3b7-43b7-c2d7-059a2f0ae55f"
   },
   "outputs": [],
   "source": [
    "# We need to install the netcdf4 libraries to read the files\n",
    "!pip install netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fl1vpo874Qeo"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "dataset = xr.open_dataset('/content/gdrive/My Drive/Datasets/gcm_run_3.nc', chunks=dict(time = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8Al76yK4Qev"
   },
   "source": [
    "Let's see the variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sRMSARWw4Qew",
    "outputId": "97a83a89-8539-4ea2-f2fe-89e6d7579889"
   },
   "outputs": [],
   "source": [
    "list(dataset.variables.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JYKEF394Qe9"
   },
   "source": [
    "- coordinates: 'time', 'lon', 'lat', 'lev'\n",
    "- prognostic variables: 'gh', 'temp', 'q', 'sp'\n",
    "- diagnostic variable: 'precnv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLGOJXKZ4QfB"
   },
   "source": [
    "Let's check the times in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "2qpbavfi4QfD",
    "outputId": "cea1058e-ca19-4ffa-bed3-345c2e301c0b"
   },
   "outputs": [],
   "source": [
    "print(\"First date: \" , dataset['time'].values.min())\n",
    "print(\"Last date: \" , dataset['time'].values.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yteK64Ll4QfX"
   },
   "source": [
    "Let's see in more detail the contents of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9V2eRz84Qfc",
    "outputId": "ccf75ca3-e139-4028-99e5-f9c8734b868d"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Isw9mT0H4Qfl"
   },
   "source": [
    "In nutshell, the shape of the model prognostic variables (our predictors) :\n",
    "\n",
    "- __gh(time, lev, lat, lon)__ = **gh(1460, 8, 16, 96)** = geopotential height [m]\n",
    "- __temp(time, lev, lat, lon)__ = **temp(1460, 8, 16, 96)** = abs. temperature [degK]\n",
    "- __q(time, lev, lat, lon)__ = **q(1460, 8, 16, 96)** = specific humidity [g/Kg]\n",
    "- __sp(time, lat, lon)__ = **sp(1460, 16, 96)** = surface pressure [hPa]\n",
    "\n",
    "The shape of the diagnosed precipitation (the predicted variable) is:\n",
    "- __precnv(time, lat, lon)__ = **precnv(1460, 16, 96)** = convective precipitation [mm/day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43EiueZr4Qfo"
   },
   "outputs": [],
   "source": [
    "# Let's remove the unnecessary attributes to have cleaner outputs\n",
    "dataset.attrs=dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "McIhO1mP4Qfv"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We are going to predict the precipitation at each grid point using the information from the predictor variables in the corresponding column.\n",
    "\n",
    "Therefore, the input variables to the Neural network consist of\n",
    "\n",
    "- __Input__: $\\vec{x}= \\left\\{ gh(z), temp(z), q(z), sp \\right\\}$, with a length of $\\vec{x}$ is 8+8+8+1=25.\n",
    "- __Output__: R, the precipitation at the ground with length=1.\n",
    "\n",
    "The training samples in the dataset consist of many columns for every time step. Hence, based on the amount of data in the dataset we have:\n",
    "\n",
    "$total\\_samples=number\\_of\\_times*number\\_of\\_lats*number\\_of\\_lons$\n",
    "\n",
    "$total\\_samples=1460*16*96=2242560$\n",
    "\n",
    "**That is more than 2 million samples per year!**\n",
    "\n",
    "\n",
    "### Step 1: Collapse dimensions \n",
    "\n",
    "In the original dataset the dimensions of our variables are (time, lev, lat, lon) or (time, lat, lon).\n",
    "What we want to feed to the Neural network are arrays with shape x=(samples, variables) and y=(samples).\n",
    "\n",
    "Therefore, we have to collapse the (time, lat, lon) dimensions into a single dimension (sample).\n",
    "\n",
    "We will create a function to do that for a given dataset and save the results into a file for future use.\n",
    "Saving the preprocessed data to a file will also help to maintain the memory requirements to the minimum.\n",
    "\n",
    "But, before we create that function, let's see this how this preprocessing is done step by step using a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eaaPQaQz4Qfx",
    "outputId": "a1c6b0d7-8b0f-4b97-c953-da5b348e860c"
   },
   "outputs": [],
   "source": [
    "# Select 100 times only (20*16*96=30720 column samples)\n",
    "small_dataset = dataset.isel(time=slice(0,20))\n",
    "dict(small_dataset.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnHAHWZE4Qf7"
   },
   "source": [
    "To collapse the (time, lat, lon) dimensions into a single dimension (sample) we will use the [Dataset's stack](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.stack.html#xarray.Dataset.stack) method.\n",
    "\n",
    "This method stack any number of existing dimensions into a single new dimension. The new dimension will be added at the end, and the corresponding coordinate variables will be combined into a MultiIndex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "Sb7pvOKs4Qf-",
    "outputId": "1071e08a-13e0-45b4-c9c9-5b6e4f7ba65c"
   },
   "outputs": [],
   "source": [
    "stacked_dataset = small_dataset.stack(sample=('time', 'lat', 'lon'))\n",
    "stacked_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDRm0Jqm4QgE"
   },
   "source": [
    "Let's reorder the variables dimensions as (sample, lev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "G6ujz6_Y4QgH",
    "outputId": "ddeb3ea3-ca50-467b-ab5e-f2a5e15042a9"
   },
   "outputs": [],
   "source": [
    "stacked_dataset = stacked_dataset.transpose('sample', 'lev')\n",
    "stacked_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiHpLFun4QgO"
   },
   "source": [
    "The last step is to create the (x, y) training samples, where their dimensions are:\n",
    "- x(samples, variables)\n",
    "- y(samples)\n",
    "\n",
    "For each column, to aggregate all the variables data into a single dimension, we will use the [numpy's append](https://docs.scipy.org/doc/numpy/reference/generated/numpy.append.html) function.\n",
    "This function append new values to the end of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67R4i7LB4QgP"
   },
   "outputs": [],
   "source": [
    "# The number of dimensions of x are x(samples, variables)\n",
    "\n",
    "# \"sp\" dimensions were (time, lat, lon). After we collapse these dimensions into \"sample\"\n",
    "# it became a 1D variable.\n",
    "\n",
    "# To start appending all the column data into a the dimension variable,\n",
    "# let's reshape sp to (samples, 1)\n",
    "x = stacked_dataset['sp'].values[:,None]\n",
    "# (None is the same as np.newaxis) \n",
    "\n",
    "# Now, let's continue adding the data into the second dimension\n",
    "for variable in ['gh', 'temp', 'q']:\n",
    "    x=np.append(x, stacked_dataset[variable].values, axis=1)\n",
    "\n",
    "y = stacked_dataset['precnv'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "mKUOr0bs4Qgb",
    "outputId": "7ad8fe05-e099-4fa6-cfa5-a92da81f2657"
   },
   "outputs": [],
   "source": [
    "print('x.shape=', x.shape)\n",
    "print('y.shape=', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yzv-QJY84Qgs"
   },
   "source": [
    "Now let's create a function that implements the preprocessing step for a given dataset (netcdf file) and save the preprocessed data into a Netcdf file for further use. In this way, we only do this preprocessing once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TL_jKM9f4Qgx"
   },
   "outputs": [],
   "source": [
    "def collapse_dims(input_dataset_path, output_dataset_path):\n",
    "    \"\"\"\n",
    "    Prepare the training samples from the input dataset for the Neural Network.\n",
    "    \n",
    "    x=(samples, variables) and y=(samples).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    input_dataset_path: str\n",
    "        Path to the netcdf file with the original SPEEDY output\n",
    "    \n",
    "    output_dataset_path: str\n",
    "        Path to the output netcdf file with the preprocessed dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Since we are working in a Jupyter notebook, to make sure that we are not\n",
    "    # modifying any variable defined outside the function, \n",
    "    # we prepend an underscore to the function local variables.\n",
    "    \n",
    "    _input_dataset = xr.open_dataset(input_dataset_path)\n",
    "    \n",
    "    _stacked_dataset = _input_dataset.stack(sample=('time', 'lat', 'lon'))\n",
    "    \n",
    "    _stacked_dataset = _stacked_dataset.transpose('sample', 'lev')\n",
    "    \n",
    "    _x = _stacked_dataset['sp'].values[:,None]\n",
    "    for variable in ['gh', 'temp', 'q']:\n",
    "        _x=np.append(_x, _stacked_dataset[variable].values, axis=1)\n",
    "\n",
    "    _y = _stacked_dataset['precnv'].values\n",
    "    \n",
    "    #Create an xarray dataset to save the file to netcdf\n",
    "    _new_dataset = xr.Dataset({'x': (['samples', 'variables'],  _x),\n",
    "                             'y': (['samples'], _y)} )\n",
    "    \n",
    "    \n",
    "    #Save dataset\n",
    "    _new_dataset.to_netcdf(output_dataset_path)\n",
    "    print(f\"Preprocesses file saved: {output_dataset_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xdn-whY4Qg4"
   },
   "source": [
    "Now, let's process all the input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "id": "t6RsXwja4Qg7",
    "outputId": "70a72c27-ef99-4c6f-bf53-ee06e195a60d"
   },
   "outputs": [],
   "source": [
    "collapse_dims('/content/gdrive/My Drive/Datasets/gcm_run_3.nc', '/content/gdrive/My Drive/Datasets/train_3.nc')\n",
    "collapse_dims('/content/gdrive/My Drive/Datasets/gcm_run_4.nc', '/content/gdrive/My Drive/Datasets/train_4.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YfFjumfW4Qhe"
   },
   "source": [
    "## Train my first neural network\n",
    "\n",
    "\n",
    "### Train and validation datasets\n",
    "\n",
    "In the completa dataset we have 10 years of data. We will use year 3 (train_3.nc) as training dataset and year 4 (train_4.nc) as validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVtbKDZF4Qhj"
   },
   "outputs": [],
   "source": [
    "train_dataset = xr.open_dataset('/content/gdrive/My Drive/Datasets/train_3.nc')\n",
    "\n",
    "test_dataset = xr.open_dataset('/content/gdrive/My Drive/Datasets/train_4.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "8-1-gYxPibgO",
    "outputId": "5af59a9e-4d71-4529-902c-544cfcdad39c"
   },
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "vtRDjlTwibdJ",
    "outputId": "19669383-2f4e-475f-e206-849956f603d9"
   },
   "outputs": [],
   "source": [
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFjmVWskixeE"
   },
   "source": [
    "Let's define new variables with only the numpy arrays values of the Datasets. This will make the computation times more efficient in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yzC0YfLginML"
   },
   "outputs": [],
   "source": [
    "x_train = train_dataset['x'].values\n",
    "y_train = train_dataset['y'].values\n",
    "\n",
    "x_test = test_dataset['x'].values\n",
    "y_test = test_dataset['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwg516jKiX6U"
   },
   "source": [
    "#### Data normalization\n",
    "For an ANN to train efficiently, all input values should be on the same order of magnitude. For this purpose, for each input variable, we subtracted the mean and divided by the standard deviation, independently for each vertical level.\n",
    "\n",
    "We will train the scaler with the training data only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LAGYq6JmiceW",
    "outputId": "a9d748a3-8865-41f6-da43-f5596a23135e"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "input_scaler = StandardScaler()\n",
    "input_scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8cg1xT4jUWv"
   },
   "source": [
    "Now let's normalize the train and the tests input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ev0Xe5QPjThc"
   },
   "outputs": [],
   "source": [
    "x_train_normed = input_scaler.transform(x_train)\n",
    "x_test_normed = input_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xu1UzZ8Pj070"
   },
   "source": [
    "Now the input and the test data is ready to be used by the Neural Network! \n",
    "\n",
    "Be aware that the normalization is only done in the input data!, there is no need to normalize the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xs2CFXRC4Qhs"
   },
   "source": [
    "### Construct the Neural Network\n",
    "\n",
    "Let's construct a small Neural Network with no hidden layers to show how the training process is done and how to evaluate the NN on the test dataset.\n",
    "\n",
    "The example neural network has the following structure:\n",
    "\n",
    "- Input: $\\vec{x}$ (length 25)\n",
    "- Output: R (length 1).\n",
    "\n",
    "- Input layer (1st layer): **25 nodes, relu activation function** \n",
    "- Output layer (4th layer): **1 nodes, relu activation function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zcly4ZFd4Qht"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as keras_bck\n",
    "\n",
    "\n",
    "my_model = Sequential()\n",
    "\n",
    "# Input layer. Here we specify the input shape.\n",
    "my_model.add(Dense(units=25, activation='relu', input_dim=25)) \n",
    "\n",
    "# No hidden layers \n",
    "\n",
    "# Output layer\n",
    "my_model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "# Let's define the metric used to evaluate the performance of the Neural Network\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Root Mean Square error metric\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    y_true : ndarray\n",
    "        Expected output values\n",
    "    \n",
    "    y_pred : ndarray\n",
    "        Predicted output values by the NN\n",
    "    \"\"\"\n",
    "    return keras_bck.sqrt( keras_bck.mean(keras_bck.square(y_pred - y_true), \n",
    "                                          axis=-1)\n",
    "                         ) \n",
    "    \n",
    "\n",
    "# Compile the model\n",
    "my_model.compile(optimizer='Adam', loss='mse', \n",
    "                 metrics=[root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HQqdQRH4Qh1"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "5vMvHz3X4QiC",
    "outputId": "d9c3424e-10ce-4e35-b8f7-26a2b4c4ccfe"
   },
   "outputs": [],
   "source": [
    "train_history = my_model.fit(x=x_train_normed,y=y_train ,\n",
    "                             validation_data=(x_test_normed,y_test), \n",
    "                             epochs=10, \n",
    "                             verbose=1,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnR9yhAQqJhX"
   },
   "source": [
    "The output history (train_history variable) has a *.history* attribute with the record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "\n",
    "Let's analyze how did the training go.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "colab_type": "code",
    "id": "-SIsKb5jqIQa",
    "outputId": "238e0d22-1f25-4c01-b1ae-84257f5facb6"
   },
   "outputs": [],
   "source": [
    "train_history.history # Dictionary with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "WFYOX9I_Ep6g",
    "outputId": "d2c1fc68-212c-45f9-b29a-24ecf87d8b2b"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_history.history['root_mean_squared_error'], label=\"train\")\n",
    "plt.plot(train_history.history['val_root_mean_squared_error'], label=\"validation\")\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Evolution of model accuracy\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precip. RMSE [mm/day]')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Umqp9rHzaBo"
   },
   "source": [
    "Let's see how the precipitation patterns looks like with the trained NN.\n",
    "\n",
    "## Predicted precipitation patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPlrCTzcQ1Jq"
   },
   "outputs": [],
   "source": [
    "# Remember that the model uses the standarized values! \n",
    "predicted_precip = my_model.predict(x_test_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vdoSAmqtSXtp",
    "outputId": "f3f48d39-0ca8-4298-c278-b7253bf19640"
   },
   "outputs": [],
   "source": [
    "predicted_precip.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "ZZjcBRleHoxD",
    "outputId": "b7673db1-e20b-444d-93e3-91dd2bc520c4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select randomly a few samples. Otherwise, the plot will take to long.\n",
    "selected_indexes = np.random.random_integers(0, high=y_test.size, size=100_000)\n",
    "\n",
    "plt.scatter(y_test.ravel()[selected_indexes], \n",
    "            predicted_precip.ravel()[selected_indexes],\n",
    "            s=3)\n",
    "\n",
    "plt.title(\"True vs Predicted precipitation\")\n",
    "\n",
    "plt.xlabel('True precip. [mm/day]')\n",
    "plt.ylabel('Predicted precip. [mm/day]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1XJNjn1gS-gz"
   },
   "source": [
    "The results do not look very encouraging, right? But, consider that we only trained for 10 epochs and we use a very simple network architecture. \n",
    "\n",
    "The goal of this first part was just to familiarize with the steps to train and use a neural network. To accurately parameterize convection with Neural Networks we will need to explore different networks architectures and find the best training parameters. \n",
    "\n",
    "\n",
    "## Find the optimal neural network architecture and training parameters\n",
    "\n",
    "Now that we know how to build and train a neural network, we can evaluate different network architectures (more layers, more neurons, different activation functions, different optimizers, etc) and find the optimal training parameters.\n",
    "\n",
    "\n",
    "To be continued....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e8d53f14-be0d-4b95-bc38-8714ec0239a1",
    "_uuid": "d5209b969be98b15aa0aa3e5923f88174fc8d7fb",
    "colab_type": "text",
    "id": "Ur60WYce4QiM"
   },
   "source": [
    "# Credits\n",
    "\n",
    "- The description of the Keras library was adapted from the Keras official documentation at https://keras.io/.\n",
    "\n",
    "- Gentine, P., Pritchard, M., Rasp, S., Reinaudi, G., & Yacalis, G. ( 2018). Could machine learning break the convection parameterization deadlock? Geophysical Research Letters, 45, 5742– 5751. \n",
    "https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2018GL078202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oh4nTVnY4QiO"
   },
   "source": [
    "# License\n",
    "\n",
    "### Mit License\n",
    "\n",
    "Copyright 2019 Andres Perez Hortal\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "07-Parametrize_Convection_using_Neural_Networks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
