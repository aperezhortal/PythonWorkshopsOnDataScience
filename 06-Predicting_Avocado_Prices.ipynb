{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "This notebook took many ideas from the following Kaggle kernels:\n",
    "\n",
    "https://www.kaggle.com/hely333/explore-avocados-from-all-sides \n",
    "https://www.kaggle.com/zdeutsch/avocados-predictions-with-ml-models-keras-ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Avocado prices\n",
    "\n",
    "In this tutorial, we will analyze the avocado prices on different US cities and attempt to predict their future prices based on their type, production, and region.\n",
    "\n",
    "\n",
    "For that, we will use the [Avocado Prices dataset from Kaggle](https://www.kaggle.com/neuromusic/avocado-prices), compiled from the [Hass Avocado Board website](https://www.hassavocadoboard.com/retail/volume-and-price-data).\n",
    "\n",
    "The dataset is a table with the weekly 2015-2018 retail scan data for National retail volume (units) and price. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (**PLU**) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table.\n",
    "\n",
    "The tables columns are as follows:\n",
    "\n",
    "- **Date** : The date of the observation.\n",
    "- **AveragePrice** : the average price of a single avocado in USD.\n",
    "- **type** : conventional or organic.\n",
    "- **year** : the year of the observation (redundant information).\n",
    "- **region** : Region \n",
    "- **Total Volume** : Total number of avocados sold.\n",
    "- **4046** : Total number of avocados with PLU 4046 sold (small Hass).\n",
    "- **4225** : Total number of avocados with PLU 4225 sold (large Hass).\n",
    "- **4770** : Total number of avocados with PLU 4770 sold (extra large Hass).\n",
    "- **Total Bags** : total number of bags sold including all types.\n",
    "- **Small Bags** : total number of bags sold of small Hass.\n",
    "- **Large Bags** : total number of bags sold of large Hass.\n",
    "- **XLarge Bags** : total number of bags sold of extra large Hass.\t\n",
    "\n",
    "## Let's load the dataset first\n",
    "\n",
    "First, download the dataset zip file [from Kaggle's website](https://www.kaggle.com/neuromusic/avocado-prices/downloads/avocado.csv/1) and unzip it in the same folder where this notebook is.\n",
    "\n",
    "Then we can read the dataset with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "avocato_ds = pd.read_csv('avocado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see its contents using the [head method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html?highlight=head#pandas.DataFrame.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocato_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column and the second column contains an aritmetic progression staring from 0. These are equivalent to the row number.\n",
    "\n",
    "Now let's see the columns types and if the contains missing or non-null values using the [info method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html?highlight=info#pandas.DataFrame.info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocato_ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset info**\n",
    "\n",
    "- The number of entries (weeks observed) is 18249 \n",
    "- All the columns contains **18249 non-null objects** (equal to the number of entries). Hence, we don't have missing values.\n",
    "- We are using 1.9 MB\n",
    "- The column \"Date\" type is object, which means a string in pandas. \n",
    "\n",
    "### The date column\n",
    "\n",
    "It is more convenient to convert the \"Date\" column to a datetime object which allows arithmetic operations between different times. For this we will use Panda's [to_datetime](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html?highlight=to_datetime#pandas.to_datetime) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocato_ds['Date'] = pd.to_datetime(avocato_ds['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make sure that the rows are sorted by date using the [sort](https://pandas.pydata.org/pandas-docs/version/0.19/generated/pandas.DataFrame.sort.html) DataFrame method.\n",
    "This will become useful when we plot different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocato_ds.sort_values('Date',axis=0, ascending=True, inplace=True)\n",
    "# the axis keyword indicates along which direction to sort: the index (0) or columns (1).\n",
    "# ascending=True : sort in ascending order\n",
    "# the inplace=True keyword modifies the DataFrame in place (do not create a new object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning \n",
    "\n",
    "Let's clean the dataset a little bit. The \"Unammed: 0\" and \"year\" column do not provide useful information. We can remove them using the [drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html)  method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocato_ds.drop(['Unnamed: 0', 'year'], axis=1,inplace=True)\n",
    "# the axis keyword indicates where to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’).\n",
    "# the inplace=True keyword modifies the DataFrame in place (do not create a new object).\n",
    "avocato_ds.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis and data cleaning\n",
    "\n",
    "The exploratory data analysis (EDA) is an approach to analyze datasets to summarize their main characteristics. \n",
    "The objective of the EDA is to initial step in every data science project were you explore the characteristics of the data, find patterns or anomalies, test assumptions about the relationship between variables, etc.\n",
    "In a nutshell, the main goal is to maximize your knowledge of the dataset. \n",
    "\n",
    "Let's explore the contents of the columns containg strings columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocato_ds['region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocato_ds['type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we care the most when we buy avocados is their price. Let's start by plotting the temporal evolution of their prices.\n",
    "Let's start by plotting the average avocato prices for each type sold on the entire US (*TotalUS* region).\n",
    "We are going to select the rows in the [DataFrame by using bolean indexes](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#different-choices-for-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import matplotlib first\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_US = avocato_ds['region']=='TotalUS'\n",
    "\n",
    "select_organic = avocato_ds['type']=='organic'\n",
    "\n",
    "select_conventional = avocato_ds['type']=='conventional'\n",
    "\n",
    "# The plot DataFrame's method return a matplotlib axes instance that can be reused in other plot() calls\n",
    "ax=avocato_ds[select_US&select_organic].plot(x='Date',y='AveragePrice',\n",
    "                                             label='organic', figsize=(12,5))\n",
    "\n",
    "avocato_ds[select_US&select_conventional].plot(x='Date',y='AveragePrice',\n",
    "                                               label='conventional',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is something weird in the organic average price aorund Jun to Aug, 2015.**\n",
    "The prices drop to 1 USD and mantain that price over a few weeks.\n",
    "This is probably an error in the dataset.\n",
    "Luckly, we have the average price and total number of avocatos sold by region. We can compute our own TotalUS prices and compare it with the actual values in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's fix the TotalUS prices \n",
    "\n",
    "To obtain the Average Price over the entire US, we need to compute the total USD sold on each region,\n",
    "compute the total by region, and then divide by the total number of advocatos sold on the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the \"Total Sold\" column with the amount in USD sold each week (each row)\n",
    "avocato_ds['Total Sale']=avocato_ds['Total Volume']*avocato_ds['AveragePrice']\n",
    "\n",
    "# Let's select the main regions\n",
    "# The main regions are described in https://www.hassavocadoboard.com/retail/volume-and-price-data\n",
    "regions= ['Southeast', 'GreatLakes', 'Northeast', 'West',\n",
    "          'California',  'Plains',  'Midsouth', 'SouthCentral']\n",
    "\n",
    "# Reminder: ' | ' represents the 'or' logical operator\n",
    "select_major_regions = avocato_ds['region'].isin(regions)\n",
    "\n",
    "# select_major_regions is a boolean Series, with a True value on the rows that \n",
    "# correspond any of the main region. False otherwise.\n",
    "organic_ds = avocato_ds[select_organic & select_major_regions]\n",
    "\n",
    "# Let's create a copy of the dataset with only the features we are interested in.\n",
    "organic_ds_short = organic_ds[['Date','Total Sale', 'Total Volume']]\n",
    "   \n",
    "# We take the total value for each week\n",
    "organic_ds_short=organic_ds_short.groupby('Date').sum()\n",
    "organic_ds_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the US Average price. \n",
    "us_average_price = organic_ds_short['Total Sale']/organic_ds_short['Total Volume']\n",
    "# us_average_price is a pandas Series, with the date as index\n",
    "print(type(us_average_price))\n",
    "\n",
    "us_average_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=avocato_ds[select_US&select_organic].plot(x='Date',y='AveragePrice',\n",
    "                                             label='organic (Original)',\n",
    "                                             legend=True, figsize=(12,5))\n",
    "us_average_price.plot(ax=ax,label='organic (New)',legend=True, color='r')\n",
    "ax.set_title('Total US - Average price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solved! Well, at least partially. \n",
    "\n",
    "We still need to __fix the values__ in our DataFrame **avocato_ds**.\n",
    "\n",
    "For that, in **avocato_ds**, we need to replace the \"AveragePrice\" values on all the rows where region=\"TotalUS\" and type='organic', by the values on **us_average_price** that we just compute. \n",
    "\n",
    "Let's review the data that we need to use:\n",
    "- **us_average_price** : Series index=Date , values=AveragePrice\n",
    "- **avocato_ds** : DataFrame, index=row number, we need to replace the values in the AveragePrice column.\n",
    "\n",
    "The series are using different indexes. This make simple assignments between DataFrame columns and series not possible.\n",
    "\n",
    "Let's create an auxiliary **us_average_price** Series that uses the row numbers as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_US = avocato_ds['region']=='TotalUS'\n",
    "select_organic = avocato_ds['type']=='organic'\n",
    "\n",
    "# Let's create a new DataFrame with the columns we need.\n",
    "aux_series = avocato_ds[select_US&select_organic][['Date','AveragePrice']]\n",
    "\n",
    "# Add the another column with the row number\n",
    "aux_series['row']=aux_series.index\n",
    "\n",
    "aux_series.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Date as index\n",
    "aux_series.set_index('Date',drop=False, inplace=True)\n",
    "aux_series.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **aux_series** and **us_average_price** uses the 'Date' as indexes. \n",
    "\n",
    "Let's check that the indexes are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_series.index.equals(us_average_price.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assing the values to AveragePrice\n",
    "aux_series['AveragePrice'] = us_average_price\n",
    "aux_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the indexes are not identical, not-a-number values are assignment on non overlapping indexes.\n",
    "\n",
    "Now, we are ready to update the old AveragePrice with the new computed ones using the DataFrame's [update](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.update.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index as row\n",
    "aux_series.set_index('row',drop=False, inplace=True)\n",
    "\n",
    "# Create a copy\n",
    "avocato_ds_fix=avocato_ds.copy()\n",
    "\n",
    "# Update the AveragePrice in the corresponding rows only.\n",
    "avocato_ds_fix['AveragePrice'].update(aux_series['AveragePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_US = avocato_ds_fix['region']=='TotalUS'\n",
    "\n",
    "select_organic = avocato_ds_fix['type']=='organic'\n",
    "\n",
    "select_conventional = avocato_ds_fix['type']=='conventional'\n",
    "\n",
    "# The plot DataFrame's method return a matplotlib axes instance that can be reused in other plot() calls\n",
    "ax=avocato_ds_fix[select_US&select_organic].plot(x='Date',y='AveragePrice',\n",
    "                                             label='organic', figsize=(12,5))\n",
    "\n",
    "avocato_ds_fix[select_US&select_conventional].plot(x='Date',y='AveragePrice',\n",
    "                                               label='conventional',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### 1) Plot the Average prices for different regions and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avocato_ds_fix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Plot the Average prices over the US together with the Total Volume. Are they correlated? \n",
    "Use [plot.scatter method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Plot the Average prices over the US together with the Total Bags. Are they correlated? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting avocados prices\n",
    "\n",
    "## Explore data correlations \n",
    "\n",
    "Here we will try to predict the average avocados prices based on the information that we have in our dataset. \n",
    "As a first step, let's explore the correlation between different variables (columns) and the prices.\n",
    "\n",
    "Let's try to predict the prices for each major region only. \n",
    "For that, we make use of the [isin](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html) DataFram's method to select all the rows that corresponds to these regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select the main regions\n",
    "# The main regions are described in \n",
    "# http://web.archive.org/web/20171017162957/https://www.hassavocadoboard.com/retail/volume-and-price-data\n",
    "\n",
    "def select_major_regions(input_dataset):\n",
    "    \"\"\"\n",
    "    Return a dataset with only the major regions.\n",
    "    \"\"\"\n",
    "    regions= ['Southeast', 'GreatLakes', 'Northeast', 'West',\n",
    "              'California',  'Plains',  'Midsouth', 'SouthCentral']\n",
    "\n",
    "    selected_regions = input_dataset['region'].isin(regions)\n",
    "    return input_dataset[selected_regions]\n",
    "\n",
    "major_regions_ds = select_major_regions(avocato_ds_fix)\n",
    "major_regions_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the correlation between different correlations between different columns using Seaborn, data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install Seaborn run one of the following commands:\n",
    "\n",
    "#!conda install -y seaborn\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the correlations between the different columns in the DataFrame using the [corr](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) method.\n",
    "This method can only compute correlations between columns with numeric values. In our dataset, the \"type\" column contains strings values. \n",
    "\n",
    "To compute the correlation in that column, we will encode the string values to numerical values (0 and 1 in this case).\n",
    "To do that we will use the [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) function from the [scikit-learn library]().\n",
    "\n",
    "The Scikit-learn library provides simple and efficient tools for data mining and data analysis\n",
    "Accessible to everybody, and reusable in various contexts.\n",
    "\n",
    "Scikit-learn is a machine learning library that provides simple and efficient tools for data mining and data analysis. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install scikit-learn run one of the following commands:\n",
    "\n",
    "#!conda install -y scikit-learn\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['AveragePrice','type', 'Total Volume','Total Bags', 'region']\n",
    "\n",
    "selected_data = major_regions_ds[columns]\n",
    "\n",
    "#################################################################\n",
    "# Encode the type column\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "label = LabelEncoder() # Create enconder instance\n",
    "\n",
    "# Fit the encder to the data.\n",
    "# In this step, the LabelEnconder find a representation of the data \n",
    "# into values between 0 and 1.\n",
    "label.fit(selected_data.type.unique()) \n",
    "\n",
    "# Transform the data from string to numerical values.\n",
    "# Note: \n",
    "# Doing selected_data['cat_type']=values will set the values on a copy of a slice from a DataFrame.\n",
    "# A more efficient way yo to this is:\n",
    "selected_data=selected_data.assign(cat_type=label.transform(selected_data['type']))\n",
    "\n",
    "selected_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  calculate the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = selected_data.corr()\n",
    "correlations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then plot the correlations using the Seaborn's [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# plot the heatmap\n",
    "sns.heatmap(correlations,\n",
    "            cbar = True,  # Add colorbar\n",
    "            annot = True, # If True, write the data value in each cell. \n",
    "            fmt = '.2f',  # String formatting code for annotations. \n",
    "            annot_kws = {'size':15}) # Keyword arguments for ax.text for annotations                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap shows that:\n",
    "\n",
    "- There is some correlation between the prices and selected columns\n",
    "- There is an anti-correlation between the prices and the production (supply), as expected.\n",
    "- The Total Volume and Total Bags have a high correlation, indicating that they provide similar information (although not the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show in more detail the relationships between the columns using Seaborn's [pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(selected_data,hue='type',vars=['AveragePrice','Total Volume','Total Bags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_organic = selected_data['type']=='organic'\n",
    "my_axes=sns.pairplot(selected_data[select_organic],\n",
    "                     vars=['AveragePrice','Total Volume','Total Bags'],\n",
    "                     hue='region')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "my_axes.fig.suptitle('Organic type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_conventional = selected_data['type']=='conventional'\n",
    "my_axes= sns.pairplot(selected_data[select_conventional],\n",
    "                      vars=['AveragePrice','Total Volume','Total Bags'],\n",
    "                      hue='region')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "my_axes.fig.suptitle('Organic type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement machine learning models\n",
    "\n",
    "\n",
    "### What is machine learning?\n",
    "\n",
    "A general definition of machine Learning is:\n",
    "\n",
    "    Machine Learning is the field of study that gives computers the ability to learn\n",
    "    without being explicitly programmed. (Arthur Samuel, 1959)\n",
    "\n",
    "What that really means is that instead of building a mathematical model based in a fixed set of rules (equations) we will try to create a model from the data itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Let's create the training and testing datasets from the available data.\n",
    "Since the prices depend on the avocado type, we will create a different model for each type.\n",
    "\n",
    "First, let's extract the predictors (x) and the predicted variable (y) from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the select_major_regions function defined above.\n",
    "major_regions_ds = select_major_regions(avocato_ds_fix)\n",
    "\n",
    "# The region column contain string values. \n",
    "# To use them in a linear regression we need to encode_them into numbers\n",
    "# This time, instead of using the LabelEncoder, we will the pandas factorize method\n",
    "#\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html\n",
    "#\n",
    "numeric_labels, unique_labels = major_regions_ds['region'].factorize()\n",
    "major_regions_ds=major_regions_ds.assign(region=np.asarray(numeric_labels, dtype=float))    \n",
    "    \n",
    "# Create two independent datasets by type\n",
    "conventional_ds =  major_regions_ds[major_regions_ds['type']=='conventional']\n",
    "\n",
    "organic_ds =  major_regions_ds[major_regions_ds['type']=='organic']\n",
    "\n",
    "x_conventional = conventional_ds[['Total Volume','Total Bags', 'region']]\n",
    "y_conventional = conventional_ds['AveragePrice']\n",
    "\n",
    "x_organic = conventional_ds[['Total Volume','Total Bags', 'region']]\n",
    "y_organic = conventional_ds['AveragePrice']\n",
    "\n",
    "\n",
    "x_conventional.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataset to train the model.\n",
    "But, we also want to validate (test) how well the model will generalize (perform) to a dataset that has never seen before. \n",
    "\n",
    "To that end, we will use a cross-validation technique where we split our training dataset into two sub-sets:\n",
    "\n",
    "- train data: Data that will be used to train the model\n",
    "- validation data: Data that will be used only to validate the model\n",
    "\n",
    "In general, it can be problematic to feed the models data with values over widely different ranges. Although the model can adapt to those values, it can make the learning process for difficult. \n",
    "\n",
    "For that, we normalize the data so all the entries (predictor variables) have a similar dynamic range. \n",
    "\n",
    "A common approach is to standarize the features (predictor variables) by removing the mean and scaling to unit variance:\n",
    "\n",
    "       z = (x - u) / s\n",
    "\n",
    "where __u__ is the mean of the training samples and **s** is the standard deviation of the training samples.\n",
    "\n",
    "To do that we will use the Scikit-learn [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "For other scaling functions see:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create test and train data for each dataset\n",
    "\n",
    "(x_train_c, x_test_c, \n",
    " y_train_c, y_test_c) = train_test_split(x_conventional,\n",
    "                                         y_conventional,\n",
    "                                         test_size = 0.25,# Keep 25% of the samples for testing \n",
    "                                         shuffle=False,\n",
    "                                         random_state=42) # Do not suffle the samples\n",
    "\n",
    "(x_train_o, x_test_o, \n",
    " y_train_o, y_test_o) = train_test_split(x_organic,\n",
    "                                         y_organic,\n",
    "                                         test_size = 0.25,# Keep 25% of the samples for testing \n",
    "                                         shuffle=False,\n",
    "                                         random_state=42) # Do not suffle the samples\n",
    "\n",
    "# Scale the data\n",
    "\n",
    "organic_scaler = StandardScaler()\n",
    "organic_scaler.fit(x_train_c)\n",
    "\n",
    "conventional_scaler = StandardScaler()\n",
    "conventional_scaler.fit(x_train_o)\n",
    "\n",
    "# Now let's transfor the training and the test data using this scaling\n",
    "x_train_c = conventional_scaler.transform(x_train_c)\n",
    "x_test_c = conventional_scaler.transform(x_test_c)\n",
    "\n",
    "x_train_o = organic_scaler.transform(x_train_o)\n",
    "x_test_o = organic_scaler.transform(x_train_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models\n",
    "\n",
    "Now with the training and validation dataset that we have, we will build and validate several type of models. \n",
    "We will show how to build the model to predict the prices of conventional avocados.\n",
    "It is left as excersice for to implement the same models for the organic avocados.\n",
    "\n",
    "### Linear regresion model\n",
    "\n",
    "The most basic form of machine learning is a mutidimensional linear regression of the data. This model assumes a linear (and unique) relationship between the predictors (independent variables, typically denote with **x**) and the predicted variable (dependent variable, typically denote with __y__).\n",
    "\n",
    "```\n",
    "y = a + b0 * x0 + b1 * x1 + ... bn * xn\n",
    "```\n",
    "\n",
    "The best fit to this equation is learned from training dataset with known (**x,y**) pairs.\n",
    "\n",
    "Let's implement a Linear regresion model using the \n",
    "[LinearRegression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    ") from the Scikit-learn library.\n",
    "\n",
    "Our linear model will use as inputs the **region, Total Volume, and Total Bags** columns. We will create a different model for each Avocado type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# Create model \n",
    "conventional_lineal_model = LinearRegression()\n",
    "\n",
    "# Train it using the training dataset\n",
    "conventional_lineal_model.fit(x_train_c,y_train_c)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained the model. Let's see how well it performs in general. \n",
    "\n",
    "One way of doing that is to measure the coefficient of determination R^2 of the prediction,\n",
    "using the [score method](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score).\n",
    "The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how well it preforms on the train set\n",
    "conventional_lineal_model.score(x_train_c,y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how well it preforms on the test set\n",
    "conventional_lineal_model.score(x_test_c,y_test_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model is not performing well... Let's see in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_plots(model,x_train, x_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Scatter plot of predicted vs actual values.\n",
    "    \"\"\"\n",
    "\n",
    "    train_predict = model.predict(x_train)\n",
    "    test_predict = model.predict(x_test)\n",
    "\n",
    "    # Let's plot the predicted values \n",
    "    fig, ax=plt.subplots(figsize=(6,6))\n",
    "    ax.set_aspect('equal')\n",
    "    plt.scatter(train_predict,y_train, label='train')\n",
    "    plt.scatter(test_predict,y_test, label='test')\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.plot([0.5,2],[0.5,2])\n",
    "    \n",
    "plot_reliability_plots(conventional_lineal_model, x_train_c, x_test_c, \n",
    "                       y_train_c, y_test_c )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is somehow an expected results. During the Exploratory Data Analysis we saw that the data didn't follow linear relationships.\n",
    "\n",
    "Let's also compute the Mean Absolute Error (MAE) for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "def compute_mae(model,x_train, x_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute error between predicted and actual values.\n",
    "    \"\"\"\n",
    "\n",
    "    train_predict = model.predict(x_train)\n",
    "    test_predict = model.predict(x_test)\n",
    "\n",
    "    mae_train = mean_absolute_error(train_predict,y_train)\n",
    "    mae_test = mean_absolute_error(test_predict,y_test)\n",
    "    \n",
    "    print(f\"MAE_train: {mae_train:.2f}\")\n",
    "    print(f\"MAE_test: {mae_test:.2f}\")\n",
    "    \n",
    "    return mae_train, mae_test\n",
    "    \n",
    "print(\"Linear model: Conventional avocados\")\n",
    "compute_mae(conventional_lineal_model, x_train_c, x_test_c, \n",
    "            y_train_c, y_test_c );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise: Linear Regression model for organic avocados \n",
    "Build and test the Linear Regression model for the organic avocados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted vs actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor model\n",
    "\n",
    "Regression based on k-nearest neighbors.\n",
    "The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.\n",
    "\n",
    "The k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. The input consists of the k closest training examples in the feature space. \n",
    "\n",
    "In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "Both for classification and regression, a useful technique can be used to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> \n",
    "        <img src=\"fig/KNN_1.png\" alt=\"KNN\" style=\"width: 250px;\"/>\n",
    "        <p>Training data</p>\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"fig/KNN_2.png\" alt=\"KNN\" style=\"width: 250px;\"/>\n",
    "        <p>1-Nearest neighbors classification map</p>\n",
    "    </td>\n",
    "      <td>\n",
    "        <img src=\"fig/KNN_3.png\" alt=\"KNN\" style=\"width: 250px;\"/>\n",
    "        <p>5-Nearest neighbors classification map</p>\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_model_conv=KNeighborsRegressor(n_neighbors=3)\n",
    "knn_model_conv.fit(x_train_c,y_train_c) \n",
    "\n",
    "plot_reliability_plots(knn_model_conv, x_train_c, x_test_c, \n",
    "                       y_train_c, y_test_c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KNN model: Conventional avocados\")\n",
    "compute_mae(knn_model_conv, x_train_c, x_test_c, \n",
    "            y_train_c, y_test_c );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise: KNN model for organic avocados\n",
    "\n",
    "Build and test the Linear Regression model for the organic avocados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted vs actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor model\n",
    "\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "<img src=\"fig/random_forests.png\" alt=\"Random Forests\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest_conv = RandomForestRegressor(n_estimators=10)\n",
    "# n_estimators=number of trees in the forest\n",
    "                                           \n",
    "random_forest_conv.fit(x_train_c,y_train_c) \n",
    "\n",
    "plot_reliability_plots(random_forest_conv, x_train_c, x_test_c, \n",
    "                       y_train_c, y_test_c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random forests model: Conventional avocados\")\n",
    "compute_mae(random_forest_conv, x_train_c, x_test_c, \n",
    "            y_train_c, y_test_c );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "adaboost_conv = GradientBoostingRegressor()\n",
    "# n_estimators=number of trees in the forest\n",
    "                                           \n",
    "adaboost_conv.fit(x_train_c,y_train_c) \n",
    "\n",
    "plot_reliability_plots(adaboost_conv, x_train_c, x_test_c, \n",
    "                       y_train_c, y_test_c )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear model: Conventional avocados\")\n",
    "compute_mae(conventional_lineal_model, x_train_c, x_test_c, \n",
    "            y_train_c, y_test_c );\n",
    "\n",
    "print(\"\\nKNN model: Conventional avocados\")\n",
    "compute_mae(knn_model_conv, x_train_c, x_test_c, \n",
    "            y_train_c, y_test_c );\n",
    "\n",
    "print(\"\\nRandom forests model: Conventional avocados\")\n",
    "compute_mae(random_forest_conv, x_train_c, x_test_c, \n",
    "            y_train_c, y_test_c );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
